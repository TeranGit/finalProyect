{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MODELO DEEP LEARNING"
      ],
      "metadata": {
        "id": "lRHwD2VeOoxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cpCxSyc7Rt3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/finalProyect')"
      ],
      "metadata": {
        "id": "9lKK0yDeR6xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "BVHqQ_WWreNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ruta del archivo procesado en Google Drive\n",
        "file_path_processed = \"/content/drive/MyDrive/finalProyect/dataset_procesado.csv\"\n",
        "\n",
        "# Cargar el conjunto de datos procesado\n",
        "df_processed = pd.read_csv(file_path_processed)\n",
        "\n",
        "# Verificar las columnas disponibles en el conjunto de datos\n",
        "print(\"Columnas disponibles en el conjunto de datos:\")\n",
        "print(df_processed.columns)\n",
        "\n",
        "# Dividir el conjunto de datos en características (X) y variable objetivo (y)\n",
        "X = df_processed.drop(\"income\", axis=1)\n",
        "y = df_processed[\"income\"]\n",
        "\n",
        "# Convertir las etiquetas categóricas a valores numéricos\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Dividir el conjunto de datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Definir las columnas categóricas para One-Hot Encoding\n",
        "categorical_cols = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\n",
        "\n",
        "# Crear un transformador para aplicar One-Hot Encoding a las columnas categóricas\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(sparse=False), categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Crear un pipeline con el preprocesador y el escalador\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('scaler', StandardScaler(with_mean=False))  # Ajustar el parámetro with_mean\n",
        "])\n",
        "\n",
        "# Aplicar el pipeline al conjunto de entrenamiento y prueba\n",
        "X_train_scaled = pipeline.fit_transform(X_train)\n",
        "X_test_scaled = pipeline.transform(X_test)\n",
        "\n",
        "\n",
        "# Crear modelo de red neuronal\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train_scaled, y_train_encoded, epochs=10, batch_size=32, validation_data=(X_test_scaled, y_test_encoded))\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_probabilities = model.predict(X_test_scaled)\n",
        "\n",
        "# Obtener las clases predichas (aplicando un umbral de 0.5)\n",
        "y_pred_dl = (y_probabilities > 0.5).astype(int)\n",
        "\n",
        "# Convertir las predicciones a etiquetas\n",
        "y_pred_dl = y_pred_dl.flatten()\n",
        "\n",
        "# Convertir las etiquetas codificadas de vuelta a las etiquetas originales\n",
        "y_test_original = label_encoder.inverse_transform(y_test_encoded)\n",
        "y_pred_original = label_encoder.inverse_transform(y_pred_dl)\n",
        "\n",
        "# Evaluar el rendimiento del modelo\n",
        "accuracy_dl = accuracy_score(y_test_encoded, y_pred_dl)\n",
        "classification_rep_dl = classification_report(y_test_encoded, y_pred_dl)\n",
        "conf_matrix_dl = confusion_matrix(y_test_encoded, y_pred_dl)\n",
        "\n",
        "# Mostrar resultados del modelo de Deep Learning\n",
        "print(f\"Accuracy del modelo de Deep Learning: {accuracy_dl}\")\n",
        "print(\"\\nClassification Report del modelo de Deep Learning:\\n\", classification_rep_dl)\n",
        "print(\"\\nConfusion Matrix del modelo de Deep Learning:\\n\", conf_matrix_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h7qULgFtZKr",
        "outputId": "3c39b6be-1e29-459f-8567-cfbc51bcc3e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columnas disponibles en el conjunto de datos:\n",
            "Index(['age', 'workclass', 'fnlwgt', 'education', 'education.num',\n",
            "       'marital.status', 'occupation', 'relationship', 'race', 'sex',\n",
            "       'capital.gain', 'capital.loss', 'hours.per.week', 'native.country',\n",
            "       'income', 'capital_ratio', 'age_scaled'],\n",
            "      dtype='object')\n",
            "Epoch 1/10\n",
            "814/814 [==============================] - 5s 4ms/step - loss: 0.3596 - accuracy: 0.8320 - val_loss: 0.3261 - val_accuracy: 0.8503\n",
            "Epoch 2/10\n",
            "814/814 [==============================] - 2s 3ms/step - loss: 0.3204 - accuracy: 0.8489 - val_loss: 0.3287 - val_accuracy: 0.8462\n",
            "Epoch 3/10\n",
            "814/814 [==============================] - 2s 3ms/step - loss: 0.3142 - accuracy: 0.8529 - val_loss: 0.3255 - val_accuracy: 0.8540\n",
            "Epoch 4/10\n",
            "814/814 [==============================] - 2s 3ms/step - loss: 0.3091 - accuracy: 0.8553 - val_loss: 0.3243 - val_accuracy: 0.8528\n",
            "Epoch 5/10\n",
            "814/814 [==============================] - 3s 4ms/step - loss: 0.3047 - accuracy: 0.8575 - val_loss: 0.3259 - val_accuracy: 0.8553\n",
            "Epoch 6/10\n",
            "814/814 [==============================] - 3s 4ms/step - loss: 0.3007 - accuracy: 0.8614 - val_loss: 0.3240 - val_accuracy: 0.8523\n",
            "Epoch 7/10\n",
            "814/814 [==============================] - 2s 3ms/step - loss: 0.2969 - accuracy: 0.8615 - val_loss: 0.3223 - val_accuracy: 0.8573\n",
            "Epoch 8/10\n",
            "814/814 [==============================] - 2s 3ms/step - loss: 0.2930 - accuracy: 0.8628 - val_loss: 0.3308 - val_accuracy: 0.8531\n",
            "Epoch 9/10\n",
            "814/814 [==============================] - 2s 3ms/step - loss: 0.2899 - accuracy: 0.8652 - val_loss: 0.3260 - val_accuracy: 0.8548\n",
            "Epoch 10/10\n",
            "814/814 [==============================] - 2s 3ms/step - loss: 0.2861 - accuracy: 0.8661 - val_loss: 0.3272 - val_accuracy: 0.8571\n",
            "204/204 [==============================] - 1s 2ms/step\n",
            "Accuracy del modelo de Deep Learning: 0.8570989551321451\n",
            "\n",
            "Classification Report del modelo de Deep Learning:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.92      0.91      4988\n",
            "           1       0.72      0.65      0.68      1520\n",
            "\n",
            "    accuracy                           0.86      6508\n",
            "   macro avg       0.81      0.78      0.79      6508\n",
            "weighted avg       0.85      0.86      0.85      6508\n",
            "\n",
            "\n",
            "Confusion Matrix del modelo de Deep Learning:\n",
            " [[4597  391]\n",
            " [ 539  981]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AJUSTE DE HIPERPARAMETROS con NEPTUNE.AI"
      ],
      "metadata": {
        "id": "W3oPXLuXDrDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade neptune-client"
      ],
      "metadata": {
        "id": "2u5qHbRG1t6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neptune-client"
      ],
      "metadata": {
        "id": "EskqcTXhFIY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neptune-contrib"
      ],
      "metadata": {
        "id": "xnEXRPS67pCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neptune-tensorflow-keras"
      ],
      "metadata": {
        "id": "2KmWiOYHNfQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import neptune.new as neptune\n",
        "from neptune.new.integrations.tensorflow_keras import NeptuneCallback\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "0kWe6IwA1vem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciar la ejecución en Neptune\n",
        "run = neptune.init(\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxYzlmZTUwMy02NGJjLTQ2NGYtOWE1Yi03MTlmNTBmMjgzMzUifQ==\",\n",
        "    project=\"frubengarcia/final\",\n",
        "    name=\"Your Run Name\",  # Puedes cambiar el nombre de la ejecución según tus necesidades\n",
        "    params={\"learning_rate\": 0.001, \"optimizer\": \"Adam\"}\n",
        ")\n",
        "\n",
        "# Ruta del archivo procesado en Google Drive\n",
        "file_path_processed = \"/content/drive/MyDrive/finalProyect/dataset_procesado.csv\"\n",
        "\n",
        "# Cargar el conjunto de datos procesado\n",
        "df_processed = pd.read_csv(file_path_processed)\n",
        "\n",
        "# Dividir el conjunto de datos en características (X) y variable objetivo (y)\n",
        "X = df_processed.drop(\"income\", axis=1)\n",
        "y = df_processed[\"income\"]\n",
        "\n",
        "# Convertir las etiquetas categóricas a valores numéricos\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Dividir el conjunto de datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Definir las columnas categóricas para One-Hot Encoding\n",
        "categorical_cols = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\n",
        "\n",
        "# Crear un transformador para aplicar One-Hot Encoding a las columnas categóricas\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(sparse=False), categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Crear un pipeline con el preprocesador y el escalador\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('scaler', StandardScaler(with_mean=False))  # Ajustar el parámetro with_mean\n",
        "])\n",
        "\n",
        "# Aplicar el pipeline al conjunto de entrenamiento y prueba\n",
        "X_train_scaled = pipeline.fit_transform(X_train)\n",
        "X_test_scaled = pipeline.transform(X_test)\n",
        "\n",
        "# Crear modelo de red neuronal\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo con NeptuneCallback\n",
        "model.fit(\n",
        "    X_train_scaled,\n",
        "    y_train_encoded,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_scaled, y_test_encoded),\n",
        "    callbacks=[NeptuneCallback(run=run)]\n",
        ")\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_probabilities = model.predict(X_test_scaled)\n",
        "\n",
        "# Obtener las clases predichas (aplicando un umbral de 0.5)\n",
        "y_pred_dl = (y_probabilities > 0.5).astype(int)\n",
        "\n",
        "# Convertir las predicciones a etiquetas\n",
        "y_pred_dl = y_pred_dl.flatten()\n",
        "\n",
        "# Convertir las etiquetas codificadas de vuelta a las etiquetas originales\n",
        "y_test_original = label_encoder.inverse_transform(y_test_encoded)\n",
        "y_pred_original = label_encoder.inverse_transform(y_pred_dl)\n",
        "\n",
        "# Evaluar el rendimiento del modelo\n",
        "accuracy_dl = accuracy_score(y_test_encoded, y_pred_dl)\n",
        "classification_rep_dl = classification_report(y_test_encoded, y_pred_dl)\n",
        "conf_matrix_dl = confusion_matrix(y_test_encoded, y_pred_dl)\n",
        "\n",
        "# Registrar métricas en Neptune\n",
        "run[\"accuracy\"] = accuracy_dl\n",
        "run[\"classification_report\"] = classification_rep_dl\n",
        "run[\"confusion_matrix\"] = str(conf_matrix_dl)\n",
        "\n",
        "# Finalizar la corrida\n",
        "run.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "O-ewSWlrNzEm",
        "outputId": "fa298647-7692-40a2-aade-6d524de6ec54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'neptune.new' has no attribute 'init'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d9440e461149>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Iniciar la ejecución en Neptune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m run = neptune.init(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mapi_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxYzlmZTUwMy02NGJjLTQ2NGYtOWE1Yi03MTlmNTBmMjgzMzUifQ==\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"frubengarcia/final\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Your Run Name\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Puedes cambiar el nombre de la ejecución según tus necesidades\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'neptune.new' has no attribute 'init'"
          ]
        }
      ]
    }
  ]
}